{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55730b46",
   "metadata": {},
   "source": [
    "# 0. Problem statement\n",
    "\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    "For many incumbent operators, retaining high profitable customers is the number one business\n",
    "goal. To reduce customer churn, telecom companies need to predict which customers are at high risk of churn. In this project, you will analyze customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn, and identify the main indicators of churn.\n",
    "\n",
    "In this competition, your goal is *to build a machine learning model that is able to predict churning customers based on the features provided for their usage.*\n",
    "\n",
    "**Customer behaviour during churn:**\n",
    "\n",
    "Customers usually do not decide to switch to another competitor instantly, but rather over a\n",
    "period of time (this is especially applicable to high-value customers). In churn prediction, we\n",
    "assume that there are three phases of customer lifecycle :\n",
    "\n",
    "1. <u>The ‘good’ phase:</u> In this phase, the customer is happy with the service and behaves as usual.\n",
    "\n",
    "2. <u>The ‘action’ phase:</u> The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. It is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)\n",
    "\n",
    "3. <u>The ‘churn’ phase:</u> In this phase, the customer is said to have churned. In this case, since you are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month (September) is the ‘churn’ phase.\n",
    "\n",
    "**Evaluation**\n",
    "<br>**Goal**\n",
    "<br>It is your job to predict if a customer will churn, given the ~170 columns containing customer behavior, usage patterns, payment patterns, and other features that might be relevant. Your target variable is \"churn_probability\"\n",
    "\n",
    "**Metric**\n",
    "<br>Submissions are evaluated on Classification Accuracy between the value of the predicted value and the actual value of churn for each of the customers.\n",
    "\n",
    "**Accuracy score formula**\n",
    "\n",
    "<br>The public leaderboard is going to rank your submission against other users while the competition is active, however, once the competition is ended, the final ranks will be calculated on the private leaderboard.\n",
    "\n",
    "**Submission file format**\n",
    "<br>The file should contain a header and have the following format (CSV):\n",
    "\n",
    "**id,churn_probability**\n",
    "<br>70005,0.0\n",
    "<br>70006,1.0\n",
    "<br>70007,0.0\n",
    "<br>etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8edc3e1",
   "metadata": {},
   "source": [
    "# Steps or Summary :\n",
    "1. Import Necessary Libraries\n",
    "2. Load the Data and Understanding the Data\n",
    "    - <i>Segmentation of Columns</i>\n",
    "    - <i>Missing Value Check</i>\n",
    "    - <i>Outlier Treatment</i>\n",
    "3. Exploratory Data Analysis:\n",
    "    - <i>Univariate Analysis (One Variable at a time)</i>\n",
    "    - <i>Bivariate Analysis (Two Variable at a time)</i>\n",
    "    - <i>Multivariate Analysis (More than two Variables at a time)</i>\n",
    "4. Data Preprocessing and Data Preparation Steps\n",
    "    - <i>Data Preparation Steps : Dummy Variable Creation (One Hot Encoding)</i>\n",
    "    - <i>Train-Test Split : Splitting the Data into Training and Testing Sets</i>\n",
    "    - <i>Feature Scaling : Standard scaling </i>\n",
    "5. Building the initial model ( Model 0 )\n",
    "6. Feature Selection : Using RFE and Manual Selection methods\n",
    "7. Check and Build the models using selected features\n",
    "8. Residual Analysis of the train data\n",
    "9. Making Predictions using the final model\n",
    "10. Steps for Further Model Refinement and Optimization\n",
    "11. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d55697",
   "metadata": {},
   "source": [
    "# 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3d2d489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Structures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "### For installing missingno library, type this command in terminal\n",
    "#pip install missingno\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "#Statsmodel\n",
    "import statsmodels as sm\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,KFold,GridSearchCV,cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "#Others\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c06dcf3",
   "metadata": {},
   "source": [
    "# 2. Load the Data and Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c6c370ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMENT THIS SECTION INCASE RUNNING THIS NOTEBOOK LOCALLY\n",
    "\n",
    "#Checking the kaggle paths for the uploaded datasets\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "216ff5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INCASE RUNNING THIS LOCALLY, PASS THE RELATIVE PATH OF THE CSV FILES BELOW\n",
    "#(e.g. if files are in same folder as notebook, simple write \"train.csv\" as path)\n",
    "\n",
    "#churn_data = pd.read_csv(\"/kaggle/input/telecom-churn-case-study-hackathon-c-63/train.csv\")\n",
    "#churn_data_unseen = pd.read_csv(\"/kaggle/input/telecom-churn-case-study-hackathon-c-63/test.csv\")\n",
    "#sample = pd.read_csv(\"/kaggle/input/telecom-churn-case-study-hackathon-c-63/sample.csv\")\n",
    "#data_dict = pd.read_csv(\"/kaggle/input/telecom-churn-case-study-hackathon-c-63/data_dictionary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad46365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load and read data\n",
    "churn_data = pd.read_csv(\"train.csv\")\n",
    "churn_data_unseen = pd.read_csv(\"test.csv\")\n",
    "sample = pd.read_csv(\"sample.csv\")\n",
    "data_dict = pd.read_csv(\"data_dictionary.csv\")\n",
    "\n",
    "print(churn_data.shape)\n",
    "print(churn_data_unseen.shape)\n",
    "print(sample.shape)\n",
    "print(data_dict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae50f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "churn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c51f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data_unseen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d81751",
   "metadata": {},
   "source": [
    "### Info of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f55af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data_unseen.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bdc72b",
   "metadata": {},
   "source": [
    "### Basic summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data_unseen.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a6308",
   "metadata": {},
   "source": [
    "## Some custom defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999436ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to rename columns based on regex patterns\n",
    "def rename_columns(col_name):\n",
    "    # Patterns and replacements\n",
    "    patterns = {\n",
    "        r'_6$': '_jun',\n",
    "        r'_7$': '_jul',\n",
    "        r'_8$': '_aug'\n",
    "    }\n",
    "    \n",
    "    for pattern, replacement in patterns.items():\n",
    "        if re.search(pattern, col_name):\n",
    "            return re.sub(pattern, replacement, col_name)\n",
    "    return col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b42a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to rename columns based on regex patterns\n",
    "def rename_columns2(col_name):\n",
    "    # Define replacements for prefixes\n",
    "    if col_name.startswith('jun_'):\n",
    "        return re.sub(r'^jun_', '', col_name) + '_jun'\n",
    "    elif col_name.startswith('jul_'):\n",
    "        return re.sub(r'^jul_', '', col_name) + '_jul'\n",
    "    elif col_name.startswith('aug_'):\n",
    "        return re.sub(r'^aug_', '', col_name) + '_aug'\n",
    "    else:\n",
    "        return col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a column has the same value in all rows\n",
    "def is_column_constant_custom(df, cols , val=1):\n",
    "    return df[cols].apply(lambda x: x.dropna().nunique() <= val and x.notna().all())\n",
    "\n",
    "# Print the number of columns in the training set where all values are the same\n",
    "constant_cols_train = churn_data.columns[is_column_constant_custom(churn_data, churn_data.columns)]\n",
    "print(\"Seen Set: Number of columns where all values are the same:\", len(constant_cols_train))\n",
    "print(\"Columns with constant values in Seen Set:\", constant_cols_train.tolist())\n",
    "# Separator line\n",
    "print(\"=\"*120)\n",
    "# Print the number of columns in the test set where all values are the same\n",
    "constant_cols_test = churn_data_unseen.columns[is_column_constant_custom(churn_data_unseen, churn_data_unseen.columns)]\n",
    "print(\"Unseen Set: Number of columns where all values are the same:\", len(constant_cols_test))\n",
    "print(\"Columns with constant values in Unseen Set:\", constant_cols_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e701fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCustomColumnTypes(df, search_type='all_search', target_column=None, include_all=False,  keyword=None):\n",
    "    \"\"\"\n",
    "    Finds and returns column names in a DataFrame based on various types and patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "      The DataFrame in which to search for column names.\n",
    "    \n",
    "    - search_type: string, optional, default='all_search'\n",
    "      Specifies the type of search to perform. Can be one of:\n",
    "        - 'all_search': Search across all specified types.\n",
    "        - 'date_text': Search for columns with 'date' in their names.\n",
    "        - 'object_type': Search for columns with object data types.\n",
    "        - 'datetime': Search for columns with datetime data types.\n",
    "        - 'numerical': Search for columns with numerical data types.\n",
    "        - 'categorical': Search for columns with categorical features.\n",
    "      \n",
    "    - include_all: boolean, optional, default=False\n",
    "      If True, includes all column names in the result under the key 'all_columns'.\n",
    "    \n",
    "    - keyword: string, optional\n",
    "      A custom string pattern to match in column names. If provided, and if \n",
    "      `search_type` matches this key, the function will return columns that match the pattern.\n",
    "\n",
    "    - target_column: string, optional\n",
    "      A column name to be excluded from the results, if present. This will be removed from \n",
    "      lists of columns for each search type where applicable.\n",
    "\n",
    "    Returns:\n",
    "    - result: dict\n",
    "      A dictionary containing lists of column names based on the search type.\n",
    "      The keys of the dictionary will be:\n",
    "        - 'all_columns': List of all columns (if `include_all=True`).\n",
    "        - 'date_text_columns': Columns containing 'date' in their names.\n",
    "        - 'object_type_columns': Columns with object data types.\n",
    "        - 'datetime_columns': Columns with datetime data types.\n",
    "        - 'numerical_columns': Columns with numerical data types.\n",
    "        - 'categorical_columns': Columns identified as categorical.\n",
    "        - '{key}_columns': Columns matching the custom `key` pattern (if provided).\n",
    "    \"\"\"\n",
    "    \n",
    "    # List all column names in the DataFrame\n",
    "    all_columns = sorted(list(df.columns))\n",
    "    result = {}\n",
    "    \n",
    "    # Include all columns if specified\n",
    "    if include_all:\n",
    "        result['all_columns'] = all_columns\n",
    "\n",
    "    # Search for columns containing 'date' in their names\n",
    "    if search_type == 'all_search' or search_type == 'date_text' or search_type == 'categorical':\n",
    "        pattern = re.compile(r'date', re.IGNORECASE)\n",
    "        date_text_columns = sorted([col for col in all_columns if pattern.search(col)])\n",
    "        # Remove target_column from date_text_columns if it exists\n",
    "        if target_column in date_text_columns:\n",
    "            date_text_columns.remove(target_column)\n",
    "        result['date_text_columns'] = date_text_columns\n",
    "    \n",
    "    # Search for columns with object data types\n",
    "    if search_type == 'all_search' or search_type == 'object_type' or search_type == 'categorical':\n",
    "        object_type_columns = sorted(list(df.select_dtypes(include=['object']).columns))\n",
    "        # Remove target_column from object_type_columns if it exists\n",
    "        if target_column in object_type_columns:\n",
    "            object_type_columns.remove(target_column)\n",
    "        result['object_type_columns'] = object_type_columns\n",
    "    \n",
    "    # Search for columns with datetime data types\n",
    "    if search_type == 'all_search' or search_type == 'datetime' or search_type == 'categorical':\n",
    "        datetime_columns = sorted(list(df.select_dtypes(include=['datetime64[ns]', '<M8[ns]']).columns))\n",
    "        # Remove target_column from datetime_columns if it exists\n",
    "        if target_column in datetime_columns:\n",
    "            datetime_columns.remove(target_column)\n",
    "        result['datetime_columns'] = datetime_columns\n",
    "    \n",
    "    # Search for numerical columns\n",
    "    if search_type == 'all_search' or search_type == 'numerical' or search_type == 'categorical':\n",
    "        numerical_columns = sorted(list(df.select_dtypes(include=['int64', 'float64']).columns))\n",
    "        # Remove target_column from numerical_columns if it exists\n",
    "        if target_column in numerical_columns:\n",
    "            numerical_columns.remove(target_column)\n",
    "        result['numerical_columns'] = numerical_columns\n",
    "    \n",
    "    # Search for categorical columns based on unique values\n",
    "    if search_type == 'all_search' or search_type == 'categorical':\n",
    "        constant_cols_train = df.columns[is_column_constant_custom(df, df.columns, 7)]\n",
    "        categorical_columns = sorted(list(set(constant_cols_train) - set(numerical_columns) - set(object_type_columns) - set(datetime_columns)))\n",
    "        # Remove target_column from categorical_columns if it exists\n",
    "        if target_column in categorical_columns:\n",
    "            categorical_columns.remove(target_column)\n",
    "        result['categorical_columns'] = categorical_columns\n",
    "    \n",
    "    # Search for custom key in column names\n",
    "    if keyword:\n",
    "        if search_type != 'all_search' or search_type == \"key\":\n",
    "            pattern = re.compile(keyword, re.IGNORECASE)\n",
    "            filtered_columns = sorted([col for col in all_columns if pattern.search(col)])\n",
    "            # Remove target_column from filtered_columns if it exists\n",
    "            if target_column in filtered_columns:\n",
    "                filtered_columns.remove(target_column)\n",
    "            result[f'{keyword}_columns'] = filtered_columns\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMissingValues(data, missing_cutoff_value, dataset_type=\"Seen\", vars_type=\"Numerical\"):\n",
    "    \"\"\"\n",
    "    Identify and print columns with missing values above a specified cutoff percentage.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The dataset to analyze.\n",
    "    missing_cutoff_value (float): The cutoff percentage for missing values to filter columns.\n",
    "    dataset_type (str): Type of dataset, e.g., \"Seen\" or \"Unseen\". Default is \"Seen\".\n",
    "    vars_type (str): Type of variables, e.g., \"Numerical\" or \"Categorical\". Default is \"Numerical\".\n",
    "\n",
    "    Returns:\n",
    "    list: List of column names with missing values above the cutoff percentage.\n",
    "    \"\"\"\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    missing_data = round(100 * data.isnull().mean())\n",
    "    # Filter columns where missing values exceed the cutoff percentage and sort them in descending order\n",
    "    missing_data_above_cutoff = missing_data[missing_data > missing_cutoff_value].sort_values(ascending=False)\n",
    "    # Print the columns with missing values above the cutoff percentage\n",
    "    print(f\"----- {vars_type} variables of {dataset_type} dataset having missing values above cutoff value ({missing_cutoff_value}%) : -----\\n{missing_data_above_cutoff}\")\n",
    "    # Find columns with missing values above the cutoff percentage\n",
    "    missing_data_above_cutoff_cols = list(missing_data_above_cutoff.index)\n",
    "    # Print the total number of columns with missing values above the cutoff and list of these columns\n",
    "    print(f\"Total number of {vars_type} variables of {dataset_type} dataset having missing values above cutoff value ({missing_cutoff_value}%) : {len(missing_data_above_cutoff_cols)}\")\n",
    "    #print(f\"-----  {vars_type} variables of {dataset_type} dataset having missing values : -----\\n{missing_data_above_cutoff_cols}\")\n",
    "    # Return the list of columns with missing values above the cutoff\n",
    "    return missing_data_above_cutoff_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleImputeMissingValues(data, data_unseen, cols, dec=\"constant\", var=0):\n",
    "    \"\"\"\n",
    "    Impute missing values in both training and test datasets.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The training dataset.\n",
    "    data_unseen (DataFrame): The unseen (test) dataset.\n",
    "    cols (list): List of column names to impute.\n",
    "    dec (str): Imputation strategy. Options are 'constant', 'mean', 'median', etc.\n",
    "    var (int, float, optional): Value to replace missing values with if `dec` is 'constant'. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    None: The function modifies the input DataFrames in place and prints the shapes of the datasets.\n",
    "    \"\"\"\n",
    "    # Set up the imputer based on the chosen strategy\n",
    "    if dec == \"constant\":\n",
    "        imputer = SimpleImputer(strategy=dec, fill_value=var)\n",
    "    else:\n",
    "        imputer = SimpleImputer(strategy=dec)\n",
    "    # Apply the imputer to the training dataset\n",
    "    data[cols] = imputer.fit_transform(data[cols])\n",
    "    # Apply the same imputer to the unseen (test) dataset\n",
    "    data_unseen[cols] = imputer.transform(data_unseen[cols])\n",
    "    # Print the shape of the DataFrame after imputation for the training set\n",
    "    print(\"Seen Set Shape :- \", data.shape)\n",
    "    # Separator line for clarity\n",
    "    print(\"=\" * 120)\n",
    "    # Print the shape of the DataFrame after imputation for the test set\n",
    "    print(\"Unseen Set Shape :- \", data_unseen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_rows(row, col1, col2, col3, val):\n",
    "    \"\"\"\n",
    "    Impute missing values in a row based on specified conditions.\n",
    "\n",
    "    Parameters:\n",
    "    row (pd.Series): A row from the DataFrame.\n",
    "    col1 (str): The name of the primary column.\n",
    "    col2 (str): The name of the first secondary column.\n",
    "    col3 (str): The name of the second secondary column.\n",
    "    val: The value to impute if the columns are missing.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: The updated row with imputed values.\n",
    "    \"\"\"\n",
    "    # Print the row ID\n",
    "    print(f\"Processing row with index {row.name}\")\n",
    "    \n",
    "    # Check and handle col1\n",
    "    if pd.isnull(row[col1]):\n",
    "        print(f\"Row {row.name}: {col1} is null. Imputing with value {val}.\")\n",
    "        row[col1] = val\n",
    "    else:\n",
    "        print(f\"Row {row.name}: {col1} is already not null, value - {row[col1]}\")\n",
    "        \n",
    "    # Check and handle col2\n",
    "    if pd.notnull(row[col1]) and pd.isnull(row[col2]):\n",
    "        print(f\"Row {row.name}: {col2} is null. Imputing with {col1} value {row[col1]}.\")\n",
    "        row[col2] = row[col1]\n",
    "    elif pd.isnull(row[col1]) and pd.isnull(row[col2]):\n",
    "        print(f\"Row {row.name}: Both {col1} and {col2} are null. Imputing {col2} with value {val}.\")\n",
    "        row[col2] = val\n",
    "    else:\n",
    "        print(f\"Row {row.name}: {col2} is already not null, value - {row[col2]}\")\n",
    "        \n",
    "    # Check and handle col3\n",
    "    if pd.notnull(row[col1]) and pd.isnull(row[col2]) and pd.isnull(row[col3]):\n",
    "        print(f\"Row {row.name}: {col3} is null. Imputing with {col1} value {row[col1]}.\")\n",
    "        row[col3] = row[col1]\n",
    "    elif pd.notnull(row[col2]) and pd.isnull(row[col3]):\n",
    "        print(f\"Row {row.name}: {col3} is null. Imputing with {col2} value {row[col2]}.\")\n",
    "        row[col3] = row[col2]\n",
    "    elif pd.isnull(row[col1]) and pd.isnull(row[col2]) and pd.isnull(row[col3]):\n",
    "        print(f\"Row {row.name}: All three columns are null. Imputing {col3} with value {val}.\")\n",
    "        row[col3] = val\n",
    "    else:\n",
    "        print(f\"Row {row.name}: {col3} is already not null, value - {row[col3]}\")\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73028af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, numeric_cols, lower_quantile=0.10, upper_quantile=0.90, iqr_multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Removes outliers from the specified numeric columns in the DataFrame based on the interquartile range method.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - numeric_cols (list): List of column names in df to remove outliers from.\n",
    "    - lower_quantile (float): The lower percentile threshold (default is 0.10).\n",
    "    - upper_quantile (float): The upper percentile threshold (default is 0.90).\n",
    "    - iqr_multiplier (float): The multiplier for the interquartile range to define outlier boundaries (default is 1.5).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A new DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    filtered_df = df.copy()\n",
    "    for col in numeric_cols:\n",
    "        # Calculate the quantiles\n",
    "        q1 = filtered_df[col].quantile(lower_quantile)\n",
    "        q3 = filtered_df[col].quantile(upper_quantile)\n",
    "        iqr = q3 - q1\n",
    "        # Calculate the lower and upper bounds for outliers\n",
    "        range_low = q1 - iqr_multiplier * iqr\n",
    "        range_high = q3 + iqr_multiplier * iqr\n",
    "        ### # Print the quantile boundaries for debugging\n",
    "        ### print(f\"Column: {col}\")\n",
    "        ### print(f\"  10th Percentile (Q1): {q1}\")\n",
    "        ### print(f\"  90th Percentile (Q3): {q3}\")\n",
    "        ### print(f\"  IQR: {iqr}\")\n",
    "        ### print(f\"  Lower Bound: {range_low}\")\n",
    "        ### print(f\"  Upper Bound: {range_high}\")\n",
    "        # Filter the DataFrame\n",
    "        filtered_df = filtered_df[(filtered_df[col] > range_low) & (filtered_df[col] < range_high)]\n",
    "        # Print the shape after filtering\n",
    "        print(f\"Shape after filtering column {col}: {filtered_df.shape}\")\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5177416",
   "metadata": {},
   "source": [
    "### Let's replace columns suffixes _6, _7, and _8 with jun, jul, and aug, respectively, using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87460d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the renaming function to all column names\n",
    "churn_data.rename(columns=lambda x: rename_columns(x), inplace=True)\n",
    "churn_data_unseen.rename(columns=lambda x: rename_columns(x), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4bc6cb",
   "metadata": {},
   "source": [
    "### Looking at the data we can see that there are a few columns which do not have the proper naming convention where the month is appended in the end. Lets fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the renaming function 2 to all column names\n",
    "churn_data.rename(columns=lambda x: rename_columns2(x), inplace=True)\n",
    "churn_data_unseen.rename(columns=lambda x: rename_columns2(x), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006293b",
   "metadata": {},
   "source": [
    "### Let's check info of datasets to check all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all column names in the DataFrame\n",
    "all_columns = list(churn_data.columns)\n",
    "print(\"------- Columns for Seen Set ------- \\n\",all_columns)\n",
    "# Separator line\n",
    "print(\"=\"*120) \n",
    "print(\"------- Columns for Unseen Set ------- \\n\",list(churn_data_unseen.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e90a0",
   "metadata": {},
   "source": [
    "### Check duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fc59c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "churn_data[churn_data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d888b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data_unseen[churn_data_unseen.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d18ae7",
   "metadata": {},
   "source": [
    "### To check all rows in some particular columns are null. If all rows are null then we'll remove those rows or records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc4be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of rows in the training set where all columns are missing\n",
    "print(\"Seen Set: Rows with all columns missing:\", churn_data[churn_data.isnull().all(axis=1)].shape[0])\n",
    "# Separator line\n",
    "print(\"=\"*120) \n",
    "# Print the number of rows in the test set where all columns are missing\n",
    "print(\"Unseen Set: Rows with all columns missing:\", churn_data_unseen[churn_data_unseen.isnull().all(axis=1)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6d9ad",
   "metadata": {},
   "source": [
    "### Checking for Columns with Constant Values (Considering NaN as unique value) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e7c07",
   "metadata": {},
   "source": [
    "### We'll drop \"circle_id\",\"last_date_of_month_jun\" columns from Seen and Unseen datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea8e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data.drop([\"circle_id\",\"last_date_of_month_jun\"],axis=1,inplace=True)\n",
    "churn_data_unseen.drop([\"circle_id\",\"last_date_of_month_jun\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41d9b3",
   "metadata": {},
   "source": [
    "### Segmentation of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28751598",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col='churn_probability'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86663713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "churn_data_columns=findCustomColumnTypes(df=churn_data, search_type='all_search', target_column=target_col, include_all=False,  keyword=None)\n",
    "churn_data_unseen_columns=findCustomColumnTypes(df=churn_data_unseen, search_type='all_search', target_column=target_col, include_all=False,  keyword=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd37d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify the object columns\n",
    "print(\"------- Object Columns for Seen Set ------- \\n\",churn_data_columns[\"object_type_columns\"])\n",
    "# Separator line\n",
    "print(\"=\"*120)\n",
    "print(\"------- Object Columns for Unseen Set ------- \\n\",churn_data_unseen_columns[\"object_type_columns\"])\n",
    "# Separator line\n",
    "print(\"=\"*120)\n",
    "### Identify the numerical columns\n",
    "print(\"------- Numerical Columns for Seen Set ------- \\n\",churn_data_columns[\"numerical_columns\"])\n",
    "# Separator line\n",
    "print(\"=\"*120) \n",
    "print(\"------- Numerical Columns for Unseen Set ------- \\n\",churn_data_unseen_columns[\"numerical_columns\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50c5d1",
   "metadata": {},
   "source": [
    "### Missing value check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491735de",
   "metadata": {},
   "source": [
    "### Check missing values for non-object columns or variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12521c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets check the columns with missing values greater than 40%\n",
    "cut_off=40\n",
    "missing_values_cols_seen=getMissingValues(churn_data[churn_data_columns[\"numerical_columns\"]],cut_off,\"Seen\",\"Numerical\")\n",
    "# Separator line\n",
    "print(\"=\"*120) \n",
    "missing_values_cols_unseen=getMissingValues(churn_data_unseen[churn_data_columns[\"numerical_columns\"]],cut_off,\"Unseen\",\"Numerical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae7b5d",
   "metadata": {},
   "source": [
    "### Many of these columns provide insights into customer behavior, spending, and engagement, all of which are crucial for understanding and predicting churn. Dropping them could result in the loss of valuable information that might be predictive of churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d936d",
   "metadata": {},
   "source": [
    "### Now we cannot simply remove these columns since they are important and mainly suggest that there has not been significant revenue generated from these customers. So we'll have to impute the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ff00b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "churn_data[missing_values_cols_seen].describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7dbd3",
   "metadata": {},
   "source": [
    "### Based on the above summary statistics of 27 columns, it appears that imputing the missing values by setting them to 0 is a sensible approach, rather than replacing the missing values with the mean or average.  A missing value in these columns might actually indicate that there was no usage (i.e., 0 minutes). Imputing these missing values with the mean would imply some level of usage when, in reality, there might have been none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_values_cols contains the columns to impute\n",
    "imputeCols = missing_values_cols_seen\n",
    "# Set up the imputer to replace missing values with 0\n",
    "simpleImputeMissingValues(churn_data, churn_data_unseen, imputeCols, dec=\"constant\", var=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc244b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets check the columns with missing values greater than 10%\n",
    "cut_off=10\n",
    "missing_values_cols_seen=getMissingValues(churn_data[churn_data_columns[\"numerical_columns\"]],cut_off,\"Seen\",\"Numerical\")\n",
    "# Separator line\n",
    "print(\"=\"*120) \n",
    "missing_values_cols_unseen=getMissingValues(churn_data_unseen[churn_data_columns[\"numerical_columns\"]],cut_off,\"Unseen\",\"Numerical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4467e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets check the columns with missing values \n",
    "cut_off=0\n",
    "missing_values_cols_seen=getMissingValues(churn_data[churn_data_columns[\"numerical_columns\"]],cut_off,\"Seen\",\"Numerical\")\n",
    "# Separator line\n",
    "print(\"=\"*120) \n",
    "missing_values_cols_unseen=getMissingValues(churn_data_unseen[churn_data_columns[\"numerical_columns\"]],cut_off,\"Unseen\",\"Numerical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa5cca",
   "metadata": {},
   "source": [
    "### Imputing with 0 aligns with the business logic that no usage (missing data) means 0 minutes or times. Here, setting missing values to 0 in this scenario better reflects the underlying reality of the data. So, let's impute with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac163ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_values_cols contains the columns to impute\n",
    "imputeCols = missing_values_cols_seen\n",
    "# Set up the imputer to replace missing values with 0\n",
    "simpleImputeMissingValues(churn_data, churn_data_unseen, imputeCols, dec=\"constant\", var=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a534f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets check the columns with missing values \n",
    "cut_off=0\n",
    "missing_values_cols_seen=getMissingValues(churn_data[churn_data_columns[\"numerical_columns\"]],cut_off,\"Seen\",\"Numerical\")\n",
    "# Separator line\n",
    "print(\"=\"*120) \n",
    "missing_values_cols_unseen=getMissingValues(churn_data_unseen[churn_data_columns[\"numerical_columns\"]],cut_off,\"Unseen\",\"Numerical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e3708",
   "metadata": {},
   "source": [
    "### Check missing values for object columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets check the object columns with missing values greater than cut off\n",
    "cut_off=40\n",
    "missing_values_cols_seen=getMissingValues(churn_data[churn_data_columns[\"object_type_columns\"]],cut_off,\"Seen\",\"Object\")\n",
    "# Separator line\n",
    "print(\"=\"*120) \n",
    "missing_values_cols_unseen=getMissingValues(churn_data_unseen[churn_data_columns[\"object_type_columns\"]],cut_off,\"Unseen\",\"Object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e81ebc",
   "metadata": {},
   "source": [
    "###   More than 70% data is blank in these 'date_of_last_rech_data_jun', 'date_of_last_rech_data_jul', 'date_of_last_rech_data_aug' columns , so for now we will drop these 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa935c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a copy or backup\n",
    "churn_data_copy1=churn_data.copy()\n",
    "churn_data_unseen_copy1=churn_data_unseen.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_values_cols contains the columns to impute\n",
    "imputeCols = missing_values_cols_seen\n",
    "## dropping these columns\n",
    "churn_data.drop(imputeCols,axis=1,inplace=True)\n",
    "## dropping these columns\n",
    "churn_data_unseen.drop(imputeCols,axis=1,inplace=True)\n",
    "# Print the shape of the DataFrame after imputation for the training set\n",
    "print(\"Seen Set Shape :- \", churn_data.shape)\n",
    "# Separator line for clarity\n",
    "print(\"=\" * 120)\n",
    "# Print the shape of the DataFrame after imputation for the test set\n",
    "print(\"Unseen Set Shape :- \", churn_data_unseen.shape)\n",
    "churn_data_columns=findCustomColumnTypes(churn_data,\"all_search\",target_col)\n",
    "churn_data_unseen_columns=findCustomColumnTypes(churn_data_unseen,\"all_search\",target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets check the object columns with missing values\n",
    "cut_off=0\n",
    "missing_values_cols_seen=getMissingValues(churn_data[churn_data_columns[\"object_type_columns\"]],cut_off,\"Seen\",\"Object\")\n",
    "# Separator line\n",
    "print(\"=\"*120) \n",
    "missing_values_cols_unseen=getMissingValues(churn_data_unseen[churn_data_columns[\"object_type_columns\"]],cut_off,\"Unseen\",\"Object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073a950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "churn_data[missing_values_cols_seen].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68000d41",
   "metadata": {},
   "source": [
    "### Since \"last_date_of_month_jul\" and \"last_date_of_month_aug\" represent specific reference dates, we will impute the missing values with these respective dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10813141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that need imputation\n",
    "imputeCols =['last_date_of_month_jul', 'last_date_of_month_aug']\n",
    "## dropping these columns\n",
    "churn_data.drop(imputeCols,axis=1,inplace=True)\n",
    "## dropping these columns\n",
    "churn_data_unseen.drop(imputeCols,axis=1,inplace=True)\n",
    "# Print the shape of the DataFrame after imputation for the training set\n",
    "print(\"Seen Set Shape :- \", churn_data.shape)\n",
    "# Separator line for clarity\n",
    "print(\"=\" * 120)\n",
    "# Print the shape of the DataFrame after imputation for the test set\n",
    "print(\"Unseen Set Shape :- \", churn_data_unseen.shape)\n",
    "churn_data_columns=findCustomColumnTypes(churn_data,\"all_search\",target_col)\n",
    "churn_data_unseen_columns=findCustomColumnTypes(churn_data_unseen,\"all_search\",target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48833c49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Lets check the object columns with missing values\n",
    "cut_off=0\n",
    "missing_values_cols_seen=getMissingValues(churn_data[churn_data_columns[\"object_type_columns\"]],cut_off,\"Seen\",\"Object\")\n",
    "# Separator line\n",
    "print(\"=\"*120) \n",
    "missing_values_cols_unseen=getMissingValues(churn_data_unseen[churn_data_columns[\"object_type_columns\"]],cut_off,\"Unseen\",\"Object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692aa7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that need imputation\n",
    "imputeCols = missing_values_cols_seen\n",
    "# Set up the SimpleImputer to replace missing values with the most frequent value\n",
    "simpleImputeMissingValues(churn_data, churn_data_unseen, imputeCols, dec=\"most_frequent\", var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab59d6b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "churn_data[missing_values_cols_seen].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27324c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets check the object columns with missing values \n",
    "cut_off=0\n",
    "missing_values_cols_seen=getMissingValues(churn_data,cut_off,\"Seen\",\"all types\")\n",
    "# Separator line\n",
    "print(\"=\"*120) \n",
    "missing_values_cols_unseen=getMissingValues(churn_data_unseen,cut_off,\"Unseen\",\"all types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49827e",
   "metadata": {},
   "source": [
    "### There are no missing values now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8caa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing columns which have only 0 as values in them\n",
    "zeros=list(churn_data.columns[(churn_data == 0).all()])\n",
    "churn_data[zeros].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81041a1a",
   "metadata": {},
   "source": [
    "### Let's drop the 9 columns where all values are identical (i.e., 0) for all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping these columns\n",
    "churn_data.drop(zeros,axis=1,inplace=True)\n",
    "## dropping these columns\n",
    "churn_data_unseen.drop(zeros,axis=1,inplace=True)\n",
    "# Print the shape of the DataFrame after imputation for the training set\n",
    "print(\"Seen Set Shape :- \", churn_data.shape)\n",
    "# Separator line for clarity\n",
    "print(\"=\" * 120)\n",
    "# Print the shape of the DataFrame after imputation for the test set\n",
    "print(\"Unseen Set Shape :- \", churn_data_unseen.shape)\n",
    "churn_data_columns=findCustomColumnTypes(churn_data,\"all_search\",target_col)\n",
    "churn_data_unseen_columns=findCustomColumnTypes(churn_data_unseen,\"all_search\",target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0224117a",
   "metadata": {},
   "source": [
    "### Convert the data types of date columns to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting the datatypes of dates to datetime\n",
    "for col in churn_data_columns[\"date_text_columns\"]:\n",
    "    churn_data[col] = pd.to_datetime(churn_data[col], format='%m/%d/%Y')\n",
    "    \n",
    "## converting the datatypes of dates to datetime\n",
    "for col in churn_data_unseen_columns[\"date_text_columns\"]:\n",
    "    churn_data_unseen[col] = pd.to_datetime(churn_data_unseen[col], format='%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data[churn_data_columns[\"date_text_columns\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49654e0f",
   "metadata": {},
   "source": [
    "### Identify different of Business Metrics Variables sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "887fea42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amt_columns': ['av_rech_amt_data_aug',\n",
       "  'av_rech_amt_data_jul',\n",
       "  'av_rech_amt_data_jun',\n",
       "  'last_day_rch_amt_aug',\n",
       "  'last_day_rch_amt_jul',\n",
       "  'last_day_rch_amt_jun',\n",
       "  'max_rech_amt_aug',\n",
       "  'max_rech_amt_jul',\n",
       "  'max_rech_amt_jun',\n",
       "  'total_rech_amt_aug',\n",
       "  'total_rech_amt_jul',\n",
       "  'total_rech_amt_jun']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findCustomColumnTypes(churn_data,\"key\",target_col,False,r\"amt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "349d6a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(rch|rech)_columns': []}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findCustomColumnTypes(churn_data, search_type=\"key\", target_column=target_col, include_all=False, key=\"(rch|rech)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match columns with 'date'\n",
    "pattern = re.compile(r'amt', re.IGNORECASE)\n",
    "# Use list comprehension to filter columns based on the pattern\n",
    "amount_columns = [col for col in list(churn_data.columns) if pattern.search(col)]\n",
    "\n",
    "# Define a regular expression pattern to match columns with 'date'\n",
    "pattern = re.compile(r'(rch|rech)', re.IGNORECASE)\n",
    "# Use list comprehension to filter columns based on the pattern\n",
    "recharge_columns = [col for col in list(churn_data.columns) if pattern.search(col)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5446f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(set(amount_columns) | set(recharge_columns) | set(data_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dceb0bd",
   "metadata": {},
   "source": [
    "### High Value Customer\n",
    "#### In this section, we will identify high-value customers based on business objectives.\n",
    "\n",
    "We will determine high-value customers based on their monthly recharge amount spending. Total Monthly Recharge Amount (total_month_rech_amt) is calculated by summing up the recorded recharge transactions (total_rech_amt_) with any additional or updated recharge data (total_rech_amt_data) to get a complete view of a customer's monthly spending. To do this, we will calculate the total monthly recharge amount using the following formula:\n",
    "` total_month_rech_amt = total_rech_amt + ( total_rech_data * av_rech_amt_data ) `\n",
    "\n",
    "We will also introduce a new metric that aggregates the recharge amounts across all months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data[[\"total_rech_amt_jun\",\"total_rech_data_jun\",\"av_rech_amt_data_jun\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d31aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of month suffixes\n",
    "months = ['jun', 'jul', 'aug']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13d0bd",
   "metadata": {},
   "source": [
    "### Find Total Data Recharge Amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total recharge amount data for each month in seen set\n",
    "churn_data['total_month_rech_amt_jun'] = churn_data['total_rech_amt_jun'] + churn_data['total_rech_data_jun'] * churn_data['av_rech_amt_data_jun']\n",
    "churn_data['total_month_rech_amt_jul'] = churn_data['total_rech_amt_jul'] + churn_data['total_rech_data_jul'] * churn_data['av_rech_amt_data_jul']\n",
    "churn_data['total_month_rech_amt_aug'] = churn_data['total_rech_amt_aug'] + churn_data['total_rech_data_aug'] * churn_data['av_rech_amt_data_aug']\n",
    "\n",
    "# Calculate total recharge amount data for each month in unseen set\n",
    "churn_data_unseen['total_month_rech_amt_jun'] = churn_data_unseen['total_rech_amt_jun'] + churn_data_unseen['total_rech_data_jun'] * churn_data_unseen['av_rech_amt_data_jun']\n",
    "churn_data_unseen['total_month_rech_amt_jul'] = churn_data_unseen['total_rech_amt_jul'] + churn_data_unseen['total_rech_data_jul'] * churn_data_unseen['av_rech_amt_data_jul']\n",
    "churn_data_unseen['total_month_rech_amt_aug'] = churn_data_unseen['total_rech_amt_aug'] + churn_data_unseen['total_rech_data_aug'] * churn_data_unseen['av_rech_amt_data_aug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc7c35",
   "metadata": {},
   "source": [
    "### Filtering High-Value Prepaid Customers Based on Business Objectives\n",
    "In this section, we'll focus on identifying high-value prepaid customers by considering usage-based churn. We'll analyze the recharge amounts for June and July and select the top 30% of customers as high-value based on their spending."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f72cb",
   "metadata": {},
   "source": [
    "### Calculate average spending in first two months. The first two months are the ‘good’ phase\n",
    "<u>The ‘good’ phase:</u> In this phase, the customer is happy with the service and behaves as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb142a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average recharge amount for the months of  June, and July\n",
    "# by taking the mean of the total recharge amounts for each month.\n",
    "\n",
    "# Adding a new column to churn_data DataFrame with the average recharge amount for June, and July\n",
    "churn_data[\"AVG_amt_jun_jul\"] = churn_data[[\"total_month_rech_amt_jun\", \"total_month_rech_amt_jul\"]].mean(axis=1)\n",
    "churn_data[[\"total_month_rech_amt_jun\", \"total_month_rech_amt_jul\", \"total_month_rech_amt_aug\",\"AVG_amt_jun_jul\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1935c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for the 'AVG_amt_jun_jul_aug' column with custom percentiles\n",
    "churn_data[[\"total_month_rech_amt_jun\", \"total_month_rech_amt_jul\", \"total_month_rech_amt_aug\",\"AVG_amt_jun_jul\"]].describe(percentiles=[0, 0.25, 0.50, 0.70, 0.75, 0.80, 0.85,0.90,0.95, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30569501",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# finding the cutoff value of good phase which is the 70th percentile of the good phase average recharge amounts\n",
    "churn_data_high=churn_data[ churn_data[\"AVG_amt_jun_jul\"] > churn_data[\"AVG_amt_jun_jul\"].quantile(0.7) ]\n",
    "## resetting the index\n",
    "churn_data_high.reset_index(inplace=True,drop=True)\n",
    "# Drop the \"AVG_amt_jun_jul\" column from the filtered DataFrame\n",
    "churn_data_high = churn_data_high.drop(\"AVG_amt_jun_jul\", axis=1)\n",
    "# Print the shape of the DataFrame for high-value customers\n",
    "print(\"Shape of the dataset for high-value customers:\", churn_data_high.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa12199",
   "metadata": {},
   "source": [
    "### Derive New Features :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad5d3c",
   "metadata": {},
   "source": [
    "### Find Total for each month and Mean incoming and outgoing information  across Different Call Types (Local , STD , ISD and Roaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25806e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total local minutes of usage for each month\n",
    "churn_data_high['total_loc_mou_jun'] = churn_data_high['loc_og_mou_jun'] + churn_data_high['loc_ic_mou_jun']\n",
    "churn_data_high['total_loc_mou_jul'] = churn_data_high['loc_og_mou_jul'] + churn_data_high['loc_ic_mou_jul']\n",
    "churn_data_high['total_loc_mou_aug'] = churn_data_high['loc_og_mou_aug'] + churn_data_high['loc_ic_mou_aug']\n",
    "\n",
    "# Calculate the mean of total local minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_high['mean_total_loc_mou'] = round(churn_data_high[['total_loc_mou_jun', 'total_loc_mou_jul', 'total_loc_mou_aug']].mean(axis=1),2)\n",
    "\n",
    "# Calculate the total STD minutes of usage for each month\n",
    "churn_data_high['total_std_mou_jun'] = churn_data_high['std_og_mou_jun'] + churn_data_high['std_ic_mou_jun']\n",
    "churn_data_high['total_std_mou_jul'] = churn_data_high['std_og_mou_jul'] + churn_data_high['std_ic_mou_jul']\n",
    "churn_data_high['total_std_mou_aug'] = churn_data_high['std_og_mou_aug'] + churn_data_high['std_ic_mou_aug']\n",
    "\n",
    "# Calculate the mean of total STD minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_high['mean_total_std_mou'] = round(churn_data_high[['total_std_mou_jun', 'total_std_mou_jul', 'total_std_mou_aug']].mean(axis=1),2)\n",
    "\n",
    "# Calculate the total ISD minutes of usage for each month\n",
    "churn_data_high['total_isd_mou_jun'] = churn_data_high['isd_og_mou_jun'] + churn_data_high['isd_ic_mou_jun']\n",
    "churn_data_high['total_isd_mou_jul'] = churn_data_high['isd_og_mou_jul'] + churn_data_high['isd_ic_mou_jul']\n",
    "churn_data_high['total_isd_mou_aug'] = churn_data_high['isd_og_mou_aug'] + churn_data_high['isd_ic_mou_aug']\n",
    "\n",
    "# Calculate the mean of total ISD minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_high['mean_total_isd_mou'] = round(churn_data_high[['total_isd_mou_jun', 'total_isd_mou_jul', 'total_isd_mou_aug']].mean(axis=1),2)\n",
    "\n",
    "# Calculate the total Roaming minutes of usage for each month\n",
    "churn_data_high['total_roam_mou_jun'] = churn_data_high['roam_og_mou_jun'] + churn_data_high['roam_ic_mou_jun']\n",
    "churn_data_high['total_roam_mou_jul'] = churn_data_high['roam_og_mou_jul'] + churn_data_high['roam_ic_mou_jul']\n",
    "churn_data_high['total_roam_mou_aug'] = churn_data_high['roam_og_mou_aug'] + churn_data_high['roam_ic_mou_aug']\n",
    "\n",
    "# Calculate the mean of total Roaming minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_high['mean_total_roam_mou'] = round(churn_data_high[['total_roam_mou_jun', 'total_roam_mou_jul', 'total_roam_mou_aug']].mean(axis=1),2)\n",
    "\n",
    "\n",
    "# Repeat the same calculations for churn_data_unseen DataFrame\n",
    "# Calculate the total local minutes of usage for each month\n",
    "churn_data_unseen['total_loc_mou_jun'] = churn_data_unseen['loc_og_mou_jun'] + churn_data_unseen['loc_ic_mou_jun']\n",
    "churn_data_unseen['total_loc_mou_jul'] = churn_data_unseen['loc_og_mou_jul'] + churn_data_unseen['loc_ic_mou_jul']\n",
    "churn_data_unseen['total_loc_mou_aug'] = churn_data_unseen['loc_og_mou_aug'] + churn_data_unseen['loc_ic_mou_aug']\n",
    "\n",
    "# Calculate the mean of total local minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_unseen['mean_total_loc_mou'] = round(churn_data_unseen[['total_loc_mou_jun', 'total_loc_mou_jul', 'total_loc_mou_aug']].mean(axis=1),2)\n",
    "\n",
    "# Calculate the total STD minutes of usage for each month\n",
    "churn_data_unseen['total_std_mou_jun'] = churn_data_unseen['std_og_mou_jun'] + churn_data_unseen['std_ic_mou_jun']\n",
    "churn_data_unseen['total_std_mou_jul'] = churn_data_unseen['std_og_mou_jul'] + churn_data_unseen['std_ic_mou_jul']\n",
    "churn_data_unseen['total_std_mou_aug'] = churn_data_unseen['std_og_mou_aug'] + churn_data_unseen['std_ic_mou_aug']\n",
    "\n",
    "# Calculate the mean of total STD minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_unseen['mean_total_std_mou'] = round(churn_data_unseen[['total_std_mou_jun', 'total_std_mou_jul', 'total_std_mou_aug']].mean(axis=1),2)\n",
    "\n",
    "# Calculate the total ISD minutes of usage for each month\n",
    "churn_data_unseen['total_isd_mou_jun'] = churn_data_unseen['isd_og_mou_jun'] + churn_data_unseen['isd_ic_mou_jun']\n",
    "churn_data_unseen['total_isd_mou_jul'] = churn_data_unseen['isd_og_mou_jul'] + churn_data_unseen['isd_ic_mou_jul']\n",
    "churn_data_unseen['total_isd_mou_aug'] = churn_data_unseen['isd_og_mou_aug'] + churn_data_unseen['isd_ic_mou_aug']\n",
    "\n",
    "# Calculate the mean of total ISD minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_unseen['mean_total_isd_mou'] = round(churn_data_unseen[['total_isd_mou_jun', 'total_isd_mou_jul', 'total_isd_mou_aug']].mean(axis=1),2)\n",
    "\n",
    "# Calculate the total Roaming minutes of usage for each month\n",
    "churn_data_unseen['total_roam_mou_jun'] = churn_data_unseen['roam_og_mou_jun'] + churn_data_unseen['roam_ic_mou_jun']\n",
    "churn_data_unseen['total_roam_mou_jul'] = churn_data_unseen['roam_og_mou_jul'] + churn_data_unseen['roam_ic_mou_jul']\n",
    "churn_data_unseen['total_roam_mou_aug'] = churn_data_unseen['roam_og_mou_aug'] + churn_data_unseen['roam_ic_mou_aug']\n",
    "\n",
    "# Calculate the mean of total Roaming minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_unseen['mean_total_roam_mou'] = round(churn_data_unseen[['total_roam_mou_jun', 'total_roam_mou_jul', 'total_roam_mou_aug']].mean(axis=1),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0160e84",
   "metadata": {},
   "source": [
    "### Metrics for Total ARPU for each month and Mean Value from 2G and 3G Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04faeb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total ARPU from 2G and 3G services for each month for churn_data_high\n",
    "churn_data_high['total_arpu_2g_3g_jun'] = churn_data_high['arpu_2g_jun'] + churn_data_high['arpu_3g_jun']\n",
    "churn_data_high['total_arpu_2g_3g_jul'] = churn_data_high['arpu_2g_jul'] + churn_data_high['arpu_3g_jul']\n",
    "churn_data_high['total_arpu_2g_3g_aug'] = churn_data_high['arpu_2g_aug'] + churn_data_high['arpu_3g_aug']\n",
    "\n",
    "# Calculate the mean of total ARPU from 2G and 3G services across June, July, and August for churn_data_high\n",
    "churn_data_high['mean_total_arpu_2g_3g'] = round(churn_data_high[['total_arpu_2g_3g_jun', 'total_arpu_2g_3g_jul', 'total_arpu_2g_3g_aug']].mean(axis=1), 2)\n",
    "\n",
    "# Calculate the total ARPU from 2G and 3G services for each month for churn_data_unseen\n",
    "churn_data_unseen['total_arpu_2g_3g_jun'] = churn_data_unseen['arpu_2g_jun'] + churn_data_unseen['arpu_3g_jun']\n",
    "churn_data_unseen['total_arpu_2g_3g_jul'] = churn_data_unseen['arpu_2g_jul'] + churn_data_unseen['arpu_3g_jul']\n",
    "churn_data_unseen['total_arpu_2g_3g_aug'] = churn_data_unseen['arpu_2g_aug'] + churn_data_unseen['arpu_3g_aug']\n",
    "\n",
    "# Calculate the mean of total ARPU from 2G and 3G services across June, July, and August for churn_data_unseen\n",
    "churn_data_unseen['mean_total_arpu_2g_3g'] = round(churn_data_unseen[['total_arpu_2g_3g_jun', 'total_arpu_2g_3g_jul', 'total_arpu_2g_3g_aug']].mean(axis=1), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e0266",
   "metadata": {},
   "source": [
    "### Metrics for Total Data Volume for each month and Mean Volume from 2G and 3G Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e14cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total data volume from 2G and 3G services for each month for churn_data_high\n",
    "churn_data_high['total_vol_2g_3g_mb_jun'] = churn_data_high['vol_2g_mb_jun'] + churn_data_high['vol_3g_mb_jun']\n",
    "churn_data_high['total_vol_2g_3g_mb_jul'] = churn_data_high['vol_2g_mb_jul'] + churn_data_high['vol_3g_mb_jul']\n",
    "churn_data_high['total_vol_2g_3g_mb_aug'] = churn_data_high['vol_2g_mb_aug'] + churn_data_high['vol_3g_mb_aug']\n",
    "\n",
    "# Calculate the mean of total data volume from 2G and 3G services across June, July, and August for churn_data_high\n",
    "churn_data_high['mean_total_vol_2g_3g_mb'] = round(churn_data_high[['total_vol_2g_3g_mb_jun', 'total_vol_2g_3g_mb_jul', 'total_vol_2g_3g_mb_aug']].mean(axis=1), 2)\n",
    "\n",
    "# Calculate the total data volume from 2G and 3G services for each month for churn_data_unseen\n",
    "churn_data_unseen['total_vol_2g_3g_mb_jun'] = churn_data_unseen['vol_2g_mb_jun'] + churn_data_unseen['vol_3g_mb_jun']\n",
    "churn_data_unseen['total_vol_2g_3g_mb_jul'] = churn_data_unseen['vol_2g_mb_jul'] + churn_data_unseen['vol_3g_mb_jul']\n",
    "churn_data_unseen['total_vol_2g_3g_mb_aug'] = churn_data_unseen['vol_2g_mb_aug'] + churn_data_unseen['vol_3g_mb_aug']\n",
    "\n",
    "# Calculate the mean of total data volume from 2G and 3G services across June, July, and August for churn_data_unseen\n",
    "churn_data_unseen['mean_total_vol_2g_3g_mb'] = round(churn_data_unseen[['total_vol_2g_3g_mb_jun', 'total_vol_2g_3g_mb_jul', 'total_vol_2g_3g_mb_aug']].mean(axis=1), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739b415",
   "metadata": {},
   "source": [
    "### Metrics for Total On-net and Off-net Minutes of Usage for each Month and Mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfa1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total on-net and off-net minutes of usage for each month for churn_data_high\n",
    "churn_data_high['total_onnet_offnet_mou_jun'] = churn_data_high['onnet_mou_jun'] + churn_data_high['offnet_mou_jun']\n",
    "churn_data_high['total_onnet_offnet_mou_jul'] = churn_data_high['onnet_mou_jul'] + churn_data_high['offnet_mou_jul']\n",
    "churn_data_high['total_onnet_offnet_mou_aug'] = churn_data_high['onnet_mou_aug'] + churn_data_high['offnet_mou_aug']\n",
    "\n",
    "# Calculate the mean of total on-net and off-net minutes of usage across June, July, and August for churn_data_high\n",
    "churn_data_high['mean_total_onnet_offnet_mou'] = round(churn_data_high[['total_onnet_offnet_mou_jun', 'total_onnet_offnet_mou_jul', 'total_onnet_offnet_mou_aug']].mean(axis=1), 2)\n",
    "\n",
    "# Calculate the total on-net and off-net minutes of usage for each month for churn_data_unseen\n",
    "churn_data_unseen['total_onnet_offnet_mou_jun'] = churn_data_unseen['onnet_mou_jun'] + churn_data_unseen['offnet_mou_jun']\n",
    "churn_data_unseen['total_onnet_offnet_mou_jul'] = churn_data_unseen['onnet_mou_jul'] + churn_data_unseen['offnet_mou_jul']\n",
    "churn_data_unseen['total_onnet_offnet_mou_aug'] = churn_data_unseen['onnet_mou_aug'] + churn_data_unseen['offnet_mou_aug']\n",
    "\n",
    "# Calculate the mean of total on-net and off-net minutes of usage across June, July, and August for churn_data_unseen\n",
    "churn_data_unseen['mean_total_onnet_offnet_mou'] = round(churn_data_unseen[['total_onnet_offnet_mou_jun', 'total_onnet_offnet_mou_jul', 'total_onnet_offnet_mou_aug']].mean(axis=1), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab1dd9",
   "metadata": {},
   "source": [
    "### Identify \"t2t\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e772442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regex pattern to match columns containing \"t2t\"\n",
    "pattern = r't2t'\n",
    "# Create a list to hold columns containing \"t2t\"\n",
    "t2t_cols = sorted([col for col in churn_data_high.columns if re.search(pattern, col, re.IGNORECASE)])\n",
    "# Print identified columns\n",
    "print('Columns with \"t2t\":', t2t_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f46c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total local T2T minutes of usage for each month\n",
    "churn_data_high['total_loc_t2t_mou_jun'] = churn_data_high['loc_og_t2t_mou_jun'] + churn_data_high['loc_ic_t2t_mou_jun']\n",
    "churn_data_high['total_loc_t2t_mou_jul'] = churn_data_high['loc_og_t2t_mou_jul'] + churn_data_high['loc_ic_t2t_mou_jul']\n",
    "churn_data_high['total_loc_t2t_mou_aug'] = churn_data_high['loc_og_t2t_mou_aug'] + churn_data_high['loc_ic_t2t_mou_aug']\n",
    "\n",
    "# Calculate the mean of total local T2T minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_high['mean_total_loc_t2t_mou'] = round(churn_data_high[['total_loc_t2t_mou_jun', 'total_loc_t2t_mou_jul', 'total_loc_t2t_mou_aug']].mean(axis=1), 2)\n",
    "\n",
    "# Calculate the total STD T2T minutes of usage for each month\n",
    "churn_data_high['total_std_t2t_mou_jun'] = churn_data_high['std_og_t2t_mou_jun'] + churn_data_high['std_ic_t2t_mou_jun']\n",
    "churn_data_high['total_std_t2t_mou_jul'] = churn_data_high['std_og_t2t_mou_jul'] + churn_data_high['std_ic_t2t_mou_jul']\n",
    "churn_data_high['total_std_t2t_mou_aug'] = churn_data_high['std_og_t2t_mou_aug'] + churn_data_high['std_ic_t2t_mou_aug']\n",
    "\n",
    "# Calculate the mean of total STD T2T minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_high['mean_total_std_t2t_mou'] = round(churn_data_high[['total_std_t2t_mou_jun', 'total_std_t2t_mou_jul', 'total_std_t2t_mou_aug']].mean(axis=1), 2)\n",
    "\n",
    "# Calculate the total local T2T minutes of usage for each month in churn_data_unseen\n",
    "churn_data_unseen['total_loc_t2t_mou_jun'] = churn_data_unseen['loc_og_t2t_mou_jun'] + churn_data_unseen['loc_ic_t2t_mou_jun']\n",
    "churn_data_unseen['total_loc_t2t_mou_jul'] = churn_data_unseen['loc_og_t2t_mou_jul'] + churn_data_unseen['loc_ic_t2t_mou_jul']\n",
    "churn_data_unseen['total_loc_t2t_mou_aug'] = churn_data_unseen['loc_og_t2t_mou_aug'] + churn_data_unseen['loc_ic_t2t_mou_aug']\n",
    "\n",
    "# Calculate the mean of total local T2T minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_unseen['mean_total_loc_t2t_mou'] = round(churn_data_unseen[['total_loc_t2t_mou_jun', 'total_loc_t2t_mou_jul', 'total_loc_t2t_mou_aug']].mean(axis=1), 2)\n",
    "\n",
    "# Calculate the total STD T2T minutes of usage for each month in churn_data_unseen\n",
    "churn_data_unseen['total_std_t2t_mou_jun'] = churn_data_unseen['std_og_t2t_mou_jun'] + churn_data_unseen['std_ic_t2t_mou_jun']\n",
    "churn_data_unseen['total_std_t2t_mou_jul'] = churn_data_unseen['std_og_t2t_mou_jul'] + churn_data_unseen['std_ic_t2t_mou_jul']\n",
    "churn_data_unseen['total_std_t2t_mou_aug'] = churn_data_unseen['std_og_t2t_mou_aug'] + churn_data_unseen['std_ic_t2t_mou_aug']\n",
    "\n",
    "# Calculate the mean of total STD T2T minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_unseen['mean_total_std_t2t_mou'] = round(churn_data_unseen[['total_std_t2t_mou_jun', 'total_std_t2t_mou_jul', 'total_std_t2t_mou_aug']].mean(axis=1), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b1bd6",
   "metadata": {},
   "source": [
    "### Identify \"t2m\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regex pattern to match columns containing \"t2m\"\n",
    "pattern = r't2m'\n",
    "# Create a list to hold columns containing \"t2m\"\n",
    "t2m_cols = sorted([col for col in churn_data_high.columns if re.search(pattern, col, re.IGNORECASE)])\n",
    "# Print identified columns\n",
    "print('Columns with \"t2m\":', t2m_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total local t2m minutes of usage for each month\n",
    "churn_data_high['total_loc_t2m_mou_jun'] = churn_data_high['loc_og_t2m_mou_jun'] + churn_data_high['loc_ic_t2m_mou_jun']\n",
    "churn_data_high['total_loc_t2m_mou_jul'] = churn_data_high['loc_og_t2m_mou_jul'] + churn_data_high['loc_ic_t2m_mou_jul']\n",
    "churn_data_high['total_loc_t2m_mou_aug'] = churn_data_high['loc_og_t2m_mou_aug'] + churn_data_high['loc_ic_t2m_mou_aug']\n",
    "\n",
    "# Calculate the mean of total local t2m minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_high['mean_total_loc_t2m_mou'] = round(churn_data_high[['total_loc_t2m_mou_jun', 'total_loc_t2m_mou_jul', 'total_loc_t2m_mou_aug']].mean(axis=1), 2)\n",
    "\n",
    "# Calculate the total STD t2m minutes of usage for each month\n",
    "churn_data_high['total_std_t2m_mou_jun'] = churn_data_high['std_og_t2m_mou_jun'] + churn_data_high['std_ic_t2m_mou_jun']\n",
    "churn_data_high['total_std_t2m_mou_jul'] = churn_data_high['std_og_t2m_mou_jul'] + churn_data_high['std_ic_t2m_mou_jul']\n",
    "churn_data_high['total_std_t2m_mou_aug'] = churn_data_high['std_og_t2m_mou_aug'] + churn_data_high['std_ic_t2m_mou_aug']\n",
    "\n",
    "# Calculate the mean of total STD t2m minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_high['mean_total_std_t2m_mou'] = round(churn_data_high[['total_std_t2m_mou_jun', 'total_std_t2m_mou_jul', 'total_std_t2m_mou_aug']].mean(axis=1), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b25d0",
   "metadata": {},
   "source": [
    "### Identify \"t2o\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regex pattern to match columns containing \"t2o\"\n",
    "pattern = r't2o'\n",
    "# Create a list to hold columns containing \"t2o\"\n",
    "t2o_cols = sorted([col for col in churn_data_high.columns if re.search(pattern, col, re.IGNORECASE)])\n",
    "# Print identified columns\n",
    "print('Columns with \"t2o\":', t2o_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af990485",
   "metadata": {},
   "source": [
    "### Identify \"t2f\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74baa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regex pattern to match columns containing \"t2f\"\n",
    "pattern = r't2f'\n",
    "# Create a list to hold columns containing \"t2f\"\n",
    "t2f_cols = sorted([col for col in churn_data_high.columns if re.search(pattern, col, re.IGNORECASE)])\n",
    "# Print identified columns\n",
    "print('Columns with \"t2f\":', t2f_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total local T2F minutes of usage for each month\n",
    "churn_data_high['total_loc_t2f_mou_jun'] = churn_data_high['loc_og_t2f_mou_jun'] + churn_data_high['loc_ic_t2f_mou_jun']\n",
    "churn_data_high['total_loc_t2f_mou_jul'] = churn_data_high['loc_og_t2f_mou_jul'] + churn_data_high['loc_ic_t2f_mou_jul']\n",
    "churn_data_high['total_loc_t2f_mou_aug'] = churn_data_high['loc_og_t2f_mou_aug'] + churn_data_high['loc_ic_t2f_mou_aug']\n",
    "\n",
    "# Calculate the mean of total local T2F minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_high['mean_total_loc_t2f_mou'] = round(churn_data_high[['total_loc_t2f_mou_jun', 'total_loc_t2f_mou_jul', 'total_loc_t2f_mou_aug']].mean(axis=1), 2)\n",
    "\n",
    "# Calculate the total STD T2F minutes of usage for each month\n",
    "churn_data_high['total_std_t2f_mou_jun'] = churn_data_high['std_og_t2f_mou_jun'] + churn_data_high['std_ic_t2f_mou_jun']\n",
    "churn_data_high['total_std_t2f_mou_jul'] = churn_data_high['std_og_t2f_mou_jul'] + churn_data_high['std_ic_t2f_mou_jul']\n",
    "churn_data_high['total_std_t2f_mou_aug'] = churn_data_high['std_og_t2f_mou_aug'] + churn_data_high['std_ic_t2f_mou_aug']\n",
    "\n",
    "# Calculate the mean of total STD T2F minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_high['mean_total_std_t2f_mou'] = round(churn_data_high[['total_std_t2f_mou_jun', 'total_std_t2f_mou_jul', 'total_std_t2f_mou_aug']].mean(axis=1), 2)\n",
    "\n",
    "# Calculate the total local T2F minutes of usage for each month\n",
    "churn_data_unseen['total_loc_t2f_mou_jun'] = churn_data_unseen['loc_og_t2f_mou_jun'] + churn_data_unseen['loc_ic_t2f_mou_jun']\n",
    "churn_data_unseen['total_loc_t2f_mou_jul'] = churn_data_unseen['loc_og_t2f_mou_jul'] + churn_data_unseen['loc_ic_t2f_mou_jul']\n",
    "churn_data_unseen['total_loc_t2f_mou_aug'] = churn_data_unseen['loc_og_t2f_mou_aug'] + churn_data_unseen['loc_ic_t2f_mou_aug']\n",
    "\n",
    "# Calculate the mean of total local T2F minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_unseen['mean_total_loc_t2f_mou'] = round(churn_data_unseen[['total_loc_t2f_mou_jun', 'total_loc_t2f_mou_jul', 'total_loc_t2f_mou_aug']].mean(axis=1), 2)\n",
    "\n",
    "# Calculate the total STD T2F minutes of usage for each month\n",
    "churn_data_unseen['total_std_t2f_mou_jun'] = churn_data_unseen['std_og_t2f_mou_jun'] + churn_data_unseen['std_ic_t2f_mou_jun']\n",
    "churn_data_unseen['total_std_t2f_mou_jul'] = churn_data_unseen['std_og_t2f_mou_jul'] + churn_data_unseen['std_ic_t2f_mou_jul']\n",
    "churn_data_unseen['total_std_t2f_mou_aug'] = churn_data_unseen['std_og_t2f_mou_aug'] + churn_data_unseen['std_ic_t2f_mou_aug']\n",
    "\n",
    "# Calculate the mean of total STD T2F minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_unseen['mean_total_std_t2f_mou'] = round(churn_data_unseen[['total_std_t2f_mou_jun', 'total_std_t2f_mou_jul', 'total_std_t2f_mou_aug']].mean(axis=1), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab0ee1",
   "metadata": {},
   "source": [
    "### Identify \"t2c\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26779cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(findCustomColumnTypes(churn_data,\"all_search\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of total local T2C minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_high['mean_total_loc_t2c_mou'] = round(churn_data_high[t2c_cols].mean(axis=1), 2)\n",
    "# Calculate the mean of total local T2F minutes of usage across June, July, and August and round to 2 decimal places\n",
    "churn_data_unseen['mean_total_loc_t2c_mou'] = round(churn_data_high[t2c_cols].mean(axis=1), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7466b",
   "metadata": {},
   "source": [
    "### Drop actual columns from dataset for calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b64d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data_high_copy2=churn_data_high.copy()\n",
    "churn_data_unseen_copy2=churn_data_unseen.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base regex patterns without month suffix\n",
    "base_patterns = [\n",
    "    r'loc_og_mou_', r'loc_ic_mou_',\n",
    "    r'std_og_mou_', r'std_ic_mou_',\n",
    "    r'isd_og_mou_', r'isd_ic_mou_',\n",
    "    r'roam_og_mou_', r'roam_ic_mou_',\n",
    "    r'arpu_2g_', r'arpu_3g_',\n",
    "    r'vol_2g_mb_', r'vol_3g_mb_',\n",
    "    r't2t',r't2m',r't2o',r't2f',r't2c'\n",
    "]\n",
    "# Create a list to hold columns to drop\n",
    "columns_to_drop = []\n",
    "# Combine all base patterns into a single regex pattern with | (OR)\n",
    "combined_base_pattern = '|'.join(base_patterns)\n",
    "# Define regex pattern to exclude columns with \"total\"\n",
    "exclude_total_pattern = r'^(?!.*(total|average|avg|mean)).*$'\n",
    "# Find columns that match any of the base patterns\n",
    "for col in churn_data_high.columns :\n",
    "    if re.search(combined_base_pattern, col) and re.match(exclude_total_pattern, col):\n",
    "        columns_to_drop.append(col)\n",
    "print(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping these columns\n",
    "churn_data_high.drop(columns_to_drop,axis=1,inplace=True)\n",
    "## dropping these columns\n",
    "churn_data_unseen.drop(columns_to_drop,axis=1,inplace=True)\n",
    "## resetting the index\n",
    "churn_data_high.reset_index(inplace=True,drop=True)\n",
    "churn_data_unseen.reset_index(inplace=True,drop=True)\n",
    "# Print the shape of the DataFrame after imputation for the training set\n",
    "print(\"Seen Set Shape :- \", churn_data_high.shape)\n",
    "# Separator line for clarity\n",
    "print(\"=\" * 120)\n",
    "# Print the shape of the DataFrame after imputation for the test set\n",
    "print(\"Unseen Set Shape :- \", churn_data_unseen.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82049026",
   "metadata": {},
   "source": [
    "### Final Check - Missing Values on High Valued Customer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209049c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets check the object columns with missing values \n",
    "cut_off=0\n",
    "missing_values_cols_seen=getMissingValues(churn_data_high,cut_off,\"Seen\",\"all types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ceea7b",
   "metadata": {},
   "source": [
    "### Outliers treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify the numerical columns for outlier treatment\n",
    "num_cols=list(churn_data_high.select_dtypes(include=['int64', 'float64']).columns)\n",
    "num_cols.remove(target_col)\n",
    "# Dictionary to store the percentage of 0s for each column\n",
    "zero_percentages = {}\n",
    "# Loop through each numeric column to calculate the percentage of 0 values\n",
    "for col in num_cols:\n",
    "    zero_count = (churn_data_high[col] == 0).sum()\n",
    "    total_count = churn_data_high[col].shape[0]\n",
    "    zero_percentage = (zero_count / total_count) * 100\n",
    "    zero_percentages[col] = zero_percentage\n",
    "# Filter columns where the percentage of 0s is less than 40%\n",
    "filtered_num_cols = [col for col, perc in zero_percentages.items() if perc < 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data_high[[col for col in num_cols if col not in filtered_num_cols]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data_high=remove_outliers(churn_data_high,filtered_num_cols,0.05,0.95,1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c317f",
   "metadata": {},
   "source": [
    "### Check Data Imbalance in Train Set if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data imbalance in the target column i.e., 'churn_probability' column of the churn_data_high DataFrame\n",
    "round ( 100 * churn_data_high[target_col].value_counts(normalize=True) , 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f8545",
   "metadata": {},
   "source": [
    "#### The proportions are as follows:\n",
    " - 91.75% of the customers are classified as class 0 (not likely to churn).\n",
    " - 8.25% of the customers are classified as class 1 (likely to churn).\n",
    "#### Points to remember:\n",
    "1. This indicates a significant class imbalance, with a much larger proportion of customers predicted not to churn.\n",
    "2. Such imbalance may affect the performance of predictive models, as they might be biased towards the majority class (class 0).\n",
    "3. The imbalance could potentially impact model performance. \n",
    "4. We will need to address this imbalance in subsequent steps to ensure a more balanced and effective model.\n",
    "### Before scaling the data, as mentioned previously, we will handle the class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79a0a9",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f2611",
   "metadata": {},
   "source": [
    "### Update Numerical Categorical Date Columns for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcada7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify unique values in columns\n",
    "# Print the number of columns in the training set where all values are the same\n",
    "constant_cols_train = churn_data_high.columns[is_column_constant_custom(churn_data_high, churn_data_high.columns, 7)]\n",
    "print(\"Seen Set: Number of columns where all values are the same:\", len(constant_cols_train))\n",
    "print(\"Columns with constant values in Seen Set:\", constant_cols_train.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53312875",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify the numerical columns\n",
    "num_cols=list(churn_data_high.select_dtypes(include=['int64', 'float64']).columns)\n",
    "num_cols=sorted(list(set(num_cols) - set(constant_cols_train)))\n",
    "print(num_cols)\n",
    "\n",
    "# Separator\n",
    "print(\"=\"*120)\n",
    "\n",
    "### Identify the object columns\n",
    "obj_cols=sorted(list(churn_data_high.select_dtypes(include=['object']).columns))\n",
    "print(obj_cols)\n",
    "\n",
    "# Separator\n",
    "print(\"=\"*120)\n",
    "\n",
    "### Identify the date columns\n",
    "date_cols=sorted(list(churn_data_high.select_dtypes(include=['<M8[ns]']).columns))\n",
    "print(date_cols)\n",
    "\n",
    "# Separator\n",
    "print(\"=\"*120)\n",
    "\n",
    "### Identify the categorical columns\n",
    "cat_cols=sorted(list( set(constant_cols_train) - (set(num_cols) | set(obj_cols) | {target_col} | set(date_cols)) ))\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ef426",
   "metadata": {},
   "source": [
    "### Identify Columns Starting with \"total\" or \"avg\" or \"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regex pattern to match columns starting with \"total\" or \"avg\" or \"mean\"\n",
    "pattern = r'^total|^avg|^mean'\n",
    "# Create a list to hold columns that start with \"total\" or \"avg\" or \"mean\"\n",
    "spl_num_cols = sorted([col for col in churn_data_high.columns if re.match(pattern, col)])\n",
    "# Print identified columns\n",
    "print(\"Columns starting with 'total' or 'avg':\", spl_num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be590c8c",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec120e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default Seaborn style and enable grid\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5,2])\n",
    "sns.countplot(data=churn_data_high, x=target_col, palette='deep')\n",
    "plt.xlabel('Churn status', fontsize=10)\n",
    "plt.ylabel('Customer count',fontsize=10)\n",
    "plt.title('Customer counts Vs Churn Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee61a8",
   "metadata": {},
   "source": [
    "### Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "start_time = time.time()\n",
    "# Define the number of subplots (adjust according to your needs)\n",
    "num_plots = len(spl_num_cols)\n",
    "# Number of columns in the subplot grid\n",
    "fig_num_cols = 4  \n",
    "fig_num_rows = (num_plots // fig_num_cols) + (num_plots % fig_num_cols != 0)  # Calculate number of rows needed\n",
    "# Set the figure size (width, height) in inches\n",
    "fig_width = fig_num_cols * 6\n",
    "fig_height = fig_num_rows * 6\n",
    "# Create the figure\n",
    "plt.figure(figsize=(fig_width, fig_height))\n",
    "# Create subplots\n",
    "for ax, col in enumerate(spl_num_cols):\n",
    "    plt.subplot(fig_num_rows, fig_num_cols, ax + 1)\n",
    "    # Generate a plot for the current column\n",
    "    sns.distplot( churn_data_high[col] , color='#55A868' )\n",
    "    # Set the title for the current subplot\n",
    "    plt.title( f' Univariate Analysis of variable using Distplot - {col} ' )\n",
    "# Adjust subplot parameters for a tight layout\n",
    "plt.tight_layout()\n",
    "# Display the plots\n",
    "plt.show()\n",
    "# End time\n",
    "end_time = time.time()\n",
    "# Calculate the elapsed time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d09ebed",
   "metadata": {},
   "source": [
    "### Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337fa29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "start_time = time.time()\n",
    "# Define the number of subplots (adjust according to your needs)\n",
    "num_plots = len(cat_cols)\n",
    "# Number of columns in the subplot grid\n",
    "fig_num_cols = 4  \n",
    "fig_num_rows = (num_plots // fig_num_cols) + (num_plots % fig_num_cols != 0)  # Calculate number of rows needed\n",
    "# Set the figure size (width, height) in inches\n",
    "fig_width = fig_num_cols * 6\n",
    "fig_height = fig_num_rows * 6\n",
    "# Create the figure\n",
    "plt.figure(figsize=(fig_width, fig_height))\n",
    "# Loop through each column to create a subplot\n",
    "for ax , col in enumerate(cat_cols):\n",
    "    # Create a subplot for each column\n",
    "    plt.subplot(fig_num_rows, fig_num_cols, ax+ 1)\n",
    "    # Order the data\n",
    "    sort_order=sorted(churn_data_high[col].unique())\n",
    "    # Generate a  plot for the current column\n",
    "    sns.countplot( data=churn_data_high , x=col , palette='deep' ,order=sort_order)\n",
    "    # Set the title for the current subplot\n",
    "    plt.title( f'Univariate Analysis of Variable - {col}' )\n",
    "# Adjust subplot parameters for a tight layout\n",
    "plt.tight_layout()\n",
    "# Display the plots\n",
    "plt.show()\n",
    "# End time\n",
    "end_time = time.time()\n",
    "# Calculate the elapsed time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c5139",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6497746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Vs Categorical\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "# Define the number of subplots (adjust according to your needs)\n",
    "num_plots = len(spl_num_cols)\n",
    "# Number of columns in the subplot grid\n",
    "fig_num_cols = 4  \n",
    "fig_num_rows = (num_plots // fig_num_cols) + (num_plots % fig_num_cols != 0)  # Calculate number of rows needed\n",
    "# Set the figure size (width, height) in inches\n",
    "fig_width = fig_num_cols * 6\n",
    "fig_height = fig_num_rows * 6\n",
    "# Create the figure\n",
    "plt.figure(figsize=(fig_width, fig_height))\n",
    "# Loop through each column to create a subplot\n",
    "for ax, col in enumerate(spl_num_cols):\n",
    "    # Create a subplot for each column\n",
    "    plt.subplot(fig_num_rows, fig_num_cols, ax+ 1)\n",
    "    sns.boxplot(x=churn_data_high[target_col], y=churn_data_high[col], palette='deep')\n",
    "    plt.title(f' Bivariate Analysis of Variable - {col} Vs. {target_col} ')\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "# Display the plots\n",
    "plt.show()\n",
    "# End time\n",
    "end_time = time.time()\n",
    "# Calculate the elapsed time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Vs Categorical\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "# Define the number of subplots (adjust according to your needs)\n",
    "num_plots = len(cat_cols)\n",
    "# Number of columns in the subplot grid\n",
    "fig_num_cols = 4  \n",
    "fig_num_rows = (num_plots // fig_num_cols) + (num_plots % fig_num_cols != 0)  # Calculate number of rows needed\n",
    "# Set the figure size (width, height) in inches\n",
    "fig_width = fig_num_cols * 6\n",
    "fig_height = fig_num_rows * 6\n",
    "# Create the figure\n",
    "plt.figure(figsize=(fig_width, fig_height))\n",
    "# Create subplots\n",
    "for ax, col in enumerate(cat_cols):\n",
    "    # Create a subplot for each column\n",
    "    plt.subplot(fig_num_rows, fig_num_cols, ax+ 1)\n",
    "    sort_order = sorted(churn_data_high[col].unique())\n",
    "    sns.countplot(data=churn_data_high, x=col, hue=target_col, palette='deep', order=sort_order)\n",
    "    plt.title(f' Bivariate Analysis of Variable - {col} Vs. {target_col} ')\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "# Display the plots\n",
    "plt.show()\n",
    "# End time\n",
    "end_time = time.time()\n",
    "# Calculate the elapsed time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf9dc7",
   "metadata": {},
   "source": [
    "### Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa495c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of Specific Numerical Variables\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "# Define the number of subplots (adjust according to your needs)\n",
    "num_plots = len(cat_cols)\n",
    "# Number of columns in the subplot grid\n",
    "fig_num_cols = 4  \n",
    "fig_num_rows = (num_plots // fig_num_cols) + (num_plots % fig_num_cols != 0)  # Calculate number of rows needed\n",
    "# Set the figure size (width, height) in inches\n",
    "fig_width = fig_num_cols * 6\n",
    "fig_height = fig_num_rows * 6\n",
    "# Create the figure\n",
    "plt.figure(figsize=(fig_width, fig_height))\n",
    "mask = np.triu(churn_data_high[spl_num_cols].corr())\n",
    "sns.heatmap(churn_data_high[spl_num_cols].corr(),mask=mask,annot=True,cmap=\"Reds\")\n",
    "plt.title('Heatmap of Specific Numerical Variables')\n",
    "# Display the plots\n",
    "plt.show()\n",
    "# End time\n",
    "end_time = time.time()\n",
    "# Calculate the elapsed time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of Specific Numerical Variables\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "# Define the number of subplots (adjust according to your needs)\n",
    "num_plots = len(num_cols)\n",
    "# Number of columns in the subplot grid\n",
    "fig_num_cols = 4  \n",
    "fig_num_rows = (num_plots // fig_num_cols) + (num_plots % fig_num_cols != 0)  # Calculate number of rows needed\n",
    "# Set the figure size (width, height) in inches\n",
    "fig_width = fig_num_cols * 6\n",
    "fig_height = fig_num_rows * 2\n",
    "# Create the figure\n",
    "plt.figure(figsize=(fig_width, fig_height))\n",
    "mask = np.triu(churn_data_high[num_cols].corr())\n",
    "sns.heatmap(churn_data_high[num_cols].corr(),mask=mask,cmap=\"Reds\")\n",
    "plt.title('Heatmap of Numerical Variables')\n",
    "# Display the plots\n",
    "plt.show()\n",
    "# End time\n",
    "end_time = time.time()\n",
    "# Calculate the elapsed time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac50f0f",
   "metadata": {},
   "source": [
    "### Analyze August Data Usage and Recharge Amounts with Various Hue Indicators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be53f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time\n",
    "start_time = time.time()\n",
    "# Define columns for the X and Y axes for June data\n",
    "cols_x_aug = 'total_month_rech_amt_aug'  # Total recharge amount in August\n",
    "cols_y_aug = 'total_vol_2g_3g_mb_aug'    # Total data usage in AUgust\n",
    "# Define the columns to be used as hue (color) in the scatter plots for August data\n",
    "cols_hue_aug = ['churn_probability', 'fb_user_aug', 'monthly_2g_aug', 'monthly_3g_aug']\n",
    "# Create scatter plots with different hues for the selected columns\n",
    "for hue_col in cols_hue_aug:\n",
    "    sns.pairplot(churn_data_high, x_vars=cols_x_aug, y_vars=cols_y_aug, hue=hue_col, \n",
    "                 kind='scatter', palette='deep', aspect=0.8)\n",
    "    plt.title(f'Churn Probability, Social Media Usage, and 2G/3G Monthly Usage (August) - Scatter Plot with {hue_col}')\n",
    "    plt.ylabel('Total Data Used in August (MB)')\n",
    "    plt.xlabel('Total Amount Spent in August')\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "# Display the plots\n",
    "plt.show()\n",
    "# End time\n",
    "end_time = time.time()\n",
    "# Calculate the elapsed time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad163668",
   "metadata": {},
   "source": [
    "### Observations from the Bivariate Analysis :\n",
    "    \n",
    "    - Call Usage: Most users show low call usage, with some heavy users as outliers.\n",
    "    - Churn Probability: Higher usage generally correlates with a higher likelihood of churn.\n",
    "    - Outliers: Numerous outliers suggest a few users have extreme usage patterns.\n",
    "    - Monthly Comparison: Median usage is consistent across months, but individual behaviors vary.\n",
    "    - Roaming Usage: High variance in roaming usage, with heavy roamers showing more churn risk.\n",
    "    - Data Usage (2G/3G): Higher data consumption is linked to a greater chance of churn.\n",
    "    - Service Type Influence: Frequent STD and ISD callers tend to have a higher churn probability.\n",
    "    - Facebook and similar social networking sites user : Most users are not Facebook users across all three months, with a consistent trend of slightly higher churn among non-users.\n",
    "    - Monthly 2G Usage: Across June, July, and August, the majority of users have minimal 2G usage, with a higher churn rate observed among users with increased 2G usage.\n",
    "    - Monthly 3G Usage: Consistently low 3G usage across all three months, with higher 3G usage correlating with a higher churn probability.\n",
    "    - Night Pack Users: Most users do not use night packs in any of these months, with a small segment of night pack users showing a higher churn rate.\n",
    "    \n",
    "### Observations from the Multivariate Analysis:\n",
    "    - High correlation among ARPU variables in June, July, and August.\n",
    "    - Strong positive correlations among total_rech_amt variables for all months.\n",
    "    - Moderate correlation between outgoing and incoming minutes of usage.\n",
    "    - Weak correlation between total recharge numbers and ARPU.\n",
    "    - Stable user behavior across different months.\n",
    "    - Potential redundancy in variables due to high correlations.\n",
    "    - Consistent trends in total minutes of usage (total_mou) across months.\n",
    "    - Recharge and usage patterns are closely linked.\n",
    "    - Low negative correlation between some recharge and usage metrics.\n",
    "    - Higher correlations in same-category features like total_arpu and total_mou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data_high_pca="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3166045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d613898d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79aa78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3842815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb02c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8cf80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff30821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b77016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d4614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28ac6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb50cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d87de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b33d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba30f77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566364a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
